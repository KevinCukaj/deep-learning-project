{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "# Audio Super-Resolution using Deep Learning\n",
        "\n",
        "This notebook implements a deep learning model to perform audio super-resolution using a U-Net architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kN-fnx7wOfA7"
      },
      "source": [
        "## 1. Setup and Dependencies\n",
        "\n",
        "First, let's install all the required libraries and import them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "kYeYn3ghOfA8",
        "outputId": "c4393ba2-b4c1-41dd-f0b3-0864d1371417"
      },
      "outputs": [],
      "source": [
        "!pip install torch torchaudio matplotlib numpy scipy librosa soundfile\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from google.colab import drive\n",
        "import librosa\n",
        "import soundfile as sf\n",
        "from tqdm.notebook import tqdm\n",
        "import random\n",
        "import gc\n",
        "import json\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from pathlib import Path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L9tS5pHOfA8"
      },
      "source": [
        "## 2. Mount Google Drive and Set Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QSlA2He4OfA8",
        "outputId": "eb771e23-8ccc-4ec2-d42e-1a7f717cbfa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive to access your dataset\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set the path to your dataset\n",
        "DATASET_PATH = '/content/drive/MyDrive/fma_small'  # Change this to your dataset location\n",
        "RESULTS_DIR  = '/content/drive/MyDrive/audio_sr_results'  # For saving models and results\n",
        "\n",
        "# Create output directory if it doesn't exist\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZVQ1795d-6d"
      },
      "outputs": [],
      "source": [
        "# Set random seeds for reproducibility\n",
        "torch.manual_seed(42)\n",
        "random.seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1rkNDw9OfA9"
      },
      "source": [
        "## 3. Data Preparation\n",
        "\n",
        "We'll create a custom dataset class for loading and preprocessing the audio files. This includes:\n",
        "- Loading the audio files\n",
        "- Converting to mono if needed\n",
        "- Creating low-resolution (16 kHz) versions of the audio\n",
        "- Splitting the dataset into training, validation, and test sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "1zqdD3CoOfA9"
      },
      "outputs": [],
      "source": [
        "class AudioSuperResolutionDataset(Dataset):\n",
        "    def __init__(self, root_dir, segment_length=16384, sr_orig=44100, sr_low=16000, max_files=None):\n",
        "        self.root_dir = root_dir\n",
        "        self.segment_length = segment_length\n",
        "        self.sr_orig = sr_orig\n",
        "        self.sr_low = sr_low\n",
        "\n",
        "        # Recursively find all mp3 files in the directory structure\n",
        "        self.file_list = []\n",
        "        print(f\"Scanning for MP3 files in {root_dir} (recursive)...\")\n",
        "\n",
        "        # Walk through all subdirectories\n",
        "        for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "            for filename in filenames:\n",
        "                if filename.endswith('.mp3'):\n",
        "                    full_path = os.path.join(dirpath, filename)\n",
        "                    self.file_list.append(full_path)\n",
        "\n",
        "        print(f\"Found {len(self.file_list)} MP3 files\")\n",
        "\n",
        "        # Shuffle the file list to ensure randomness across folders\n",
        "        random.shuffle(self.file_list)\n",
        "\n",
        "        # Limit the number of files if specified\n",
        "        if max_files is not None:\n",
        "            self.file_list = self.file_list[:max_files]\n",
        "            print(f\"Limited dataset to {len(self.file_list)} files\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.file_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio_path = self.file_list[idx]\n",
        "\n",
        "        # Load audio file\n",
        "        try:\n",
        "            waveform, sample_rate = torchaudio.load(audio_path)\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading {audio_path}: {e}\")\n",
        "            # If there's an error, return a random valid index instead\n",
        "            return self.__getitem__(random.randint(0, len(self.file_list) - 1))\n",
        "\n",
        "        # Convert to mono if stereo\n",
        "        if waveform.shape[0] > 1:\n",
        "            waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "        # Resample if necessary\n",
        "        if sample_rate != self.sr_orig:\n",
        "            waveform = torchaudio.functional.resample(waveform, sample_rate, self.sr_orig)\n",
        "\n",
        "        # Randomly select segment\n",
        "        if waveform.shape[1] > self.segment_length:\n",
        "            start_idx = torch.randint(0, waveform.shape[1] - self.segment_length, (1,))\n",
        "            waveform = waveform[:, start_idx:start_idx + self.segment_length]\n",
        "        else:\n",
        "            # Pad if audio is shorter than segment_length\n",
        "            padding = self.segment_length - waveform.shape[1]\n",
        "            waveform = torch.nn.functional.pad(waveform, (0, padding))\n",
        "\n",
        "        # Create low resolution version with proper low-pass filtering\n",
        "        # 1. Downsample to low SR\n",
        "        waveform_low = torchaudio.functional.resample(waveform, self.sr_orig, self.sr_low)\n",
        "        # 2. Upsample back to original SR\n",
        "        waveform_low = torchaudio.functional.resample(waveform_low, self.sr_low, self.sr_orig)\n",
        "\n",
        "        # 3. Apply low-pass filter to simulate bandwidth limitation\n",
        "        cutoff_freq = self.sr_low / 2 * 0.6  # 60% of Nyquist frequency for the low sample rate\n",
        "        waveform_low = torchaudio.functional.lowpass_biquad(\n",
        "            waveform_low,\n",
        "            self.sr_orig,\n",
        "            cutoff_freq\n",
        "        )\n",
        "\n",
        "        # Ensure both waveforms have the same length after resampling and filtering\n",
        "        if waveform_low.shape[1] != self.segment_length:\n",
        "             if waveform_low.shape[1] > self.segment_length:\n",
        "                waveform_low = waveform_low[:, :self.segment_length]\n",
        "             else:\n",
        "                padding = self.segment_length - waveform_low.shape[1]\n",
        "                waveform_low = torch.nn.functional.pad(waveform_low, (0, padding))\n",
        "\n",
        "        # Normalize\n",
        "        waveform = waveform / (torch.max(torch.abs(waveform)) + 1e-8)\n",
        "        waveform_low = waveform_low / (torch.max(torch.abs(waveform_low)) + 1e-8)\n",
        "\n",
        "        return waveform_low.squeeze(0), waveform.squeeze(0)\n",
        "\n",
        "# Create dataset splits\n",
        "def create_dataset_splits(dataset_path, batch_size=8, train_ratio=0.8, val_ratio=0.1, test_ratio=0.1,\n",
        "                          segment_length=16384, sr_orig=44100, sr_low=16000, num_workers=2, seed=42,\n",
        "                          max_files=1000):  # Add max_files parameter with default of 1000\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Create the dataset with recursive file search and file limit\n",
        "    full_dataset = AudioSuperResolutionDataset(\n",
        "        dataset_path,\n",
        "        segment_length=segment_length,\n",
        "        sr_orig=sr_orig,\n",
        "        sr_low=sr_low,\n",
        "        max_files=max_files  # Pass the max_files parameter\n",
        "    )\n",
        "\n",
        "    # Split the dataset\n",
        "    dataset_size = len(full_dataset)\n",
        "    train_size = int(train_ratio * dataset_size)\n",
        "    val_size = int(val_ratio * dataset_size)\n",
        "    test_size = dataset_size - train_size - val_size\n",
        "\n",
        "    print(f\"Dataset split: {train_size} training, {val_size} validation, {test_size} test samples\")\n",
        "\n",
        "    # Create a generator for reproducible splits\n",
        "    generator = torch.Generator().manual_seed(seed)\n",
        "\n",
        "    train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(\n",
        "        full_dataset, [train_size, val_size, test_size], generator=generator\n",
        "    )\n",
        "\n",
        "    # Create data loaders with worker initialization to ensure proper randomization\n",
        "    def worker_init_fn(worker_id):\n",
        "        worker_seed = torch.initial_seed() % 2**32\n",
        "        np.random.seed(worker_seed)\n",
        "        random.seed(worker_seed)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        worker_init_fn=worker_init_fn,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnwTH1vqOfA-"
      },
      "source": [
        "## 4. Visualize Dataset Samples\n",
        "\n",
        "Let's visualize some samples from our dataset to ensure everything is working correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v2C0ASL8OfA-",
        "outputId": "f4213d48-4cb6-489d-c081-9654a82f5fef"
      },
      "outputs": [],
      "source": [
        "def visualize_samples(dataset_path, num_samples, segment_length):\n",
        "    dataset = AudioSuperResolutionDataset(dataset_path, segment_length=segment_length)\n",
        "\n",
        "    for i in range(num_samples):\n",
        "        #idx = random.randint(0, len(dataset) - 1)\n",
        "        idx = 2\n",
        "        low_res, high_res = dataset[idx]\n",
        "\n",
        "        plt.figure(figsize=(15, 6))\n",
        "\n",
        "        plt.subplot(2, 1, 1)\n",
        "        plt.plot(high_res.numpy())\n",
        "        plt.title('High Resolution (44.1 kHz)')\n",
        "        plt.xlabel('Sample')\n",
        "        plt.ylabel('Amplitude')\n",
        "\n",
        "        plt.subplot(2, 1, 2)\n",
        "        plt.plot(low_res.numpy())\n",
        "        plt.title('Low Resolution (44.1 kHz  ->  16 kHz  ->  44.1 kHz)')\n",
        "        plt.xlabel('Sample')\n",
        "        plt.ylabel('Amplitude')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        # Also plot spectrograms\n",
        "        plt.figure(figsize=(15, 6))\n",
        "\n",
        "        plt.subplot(1, 2, 1)\n",
        "        spec = librosa.stft(high_res.numpy())\n",
        "        spec_db = librosa.amplitude_to_db(np.abs(spec), ref=np.max)\n",
        "        plt.imshow(spec_db, aspect='auto', origin='lower')\n",
        "        plt.title('High Resolution Spectrogram')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "\n",
        "        # The spectrogram will apear much more \"noisy\" compared to the high-res version\n",
        "        plt.subplot(1, 2, 2)\n",
        "        spec = librosa.stft(low_res.numpy())\n",
        "        spec_db = librosa.amplitude_to_db(np.abs(spec), ref=np.max)\n",
        "        plt.imshow(spec_db, aspect='auto', origin='lower')\n",
        "        plt.title('Low Resolution Spectrogram')\n",
        "        plt.colorbar(format='%+2.0f dB')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "# Uncomment to visualize samples\n",
        "visualize_samples(DATASET_PATH, num_samples=1, segment_length=8192)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9sPIETWOfA-"
      },
      "source": [
        "## 5. Model Architecture\n",
        "\n",
        "We'll implement a U-Net architecture for audio super-resolution, which has proven effective for this task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "8bhBhYXcOfA-"
      },
      "outputs": [],
      "source": [
        "class AudioUNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(AudioUNet, self).__init__()\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = nn.Conv1d(1, 64, kernel_size=15, stride=1, padding=7)\n",
        "        self.enc2 = nn.Conv1d(64, 128, kernel_size=15, stride=2, padding=7)\n",
        "        self.enc3 = nn.Conv1d(128, 256, kernel_size=15, stride=2, padding=7)\n",
        "        self.enc4 = nn.Conv1d(256, 512, kernel_size=15, stride=2, padding=7)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec4 = nn.ConvTranspose1d(512, 256, kernel_size=16, stride=2, padding=7)\n",
        "        self.dec3 = nn.ConvTranspose1d(512, 128, kernel_size=16, stride=2, padding=7)\n",
        "        self.dec2 = nn.ConvTranspose1d(256, 64, kernel_size=16, stride=2, padding=7)\n",
        "        self.dec1 = nn.Conv1d(128, 1, kernel_size=15, stride=1, padding=7)\n",
        "\n",
        "        # Batch normalization\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(128)\n",
        "        self.bn3 = nn.BatchNorm1d(256)\n",
        "        self.bn4 = nn.BatchNorm1d(512)\n",
        "\n",
        "        self.bn_dec4 = nn.BatchNorm1d(256)\n",
        "        self.bn_dec3 = nn.BatchNorm1d(128)\n",
        "        self.bn_dec2 = nn.BatchNorm1d(64)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Add channel dimension if needed\n",
        "        if len(x.shape) == 2:\n",
        "            x = x.unsqueeze(1)\n",
        "\n",
        "        # Encoder\n",
        "        e1 = F.leaky_relu(self.bn1(self.enc1(x)), 0.2)\n",
        "        e2 = F.leaky_relu(self.bn2(self.enc2(e1)), 0.2)\n",
        "        e3 = F.leaky_relu(self.bn3(self.enc3(e2)), 0.2)\n",
        "        e4 = F.leaky_relu(self.bn4(self.enc4(e3)), 0.2)\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d4 = F.relu(self.bn_dec4(self.dec4(e4)))\n",
        "        d4 = torch.cat([d4, e3], dim=1)  # Skip connection\n",
        "\n",
        "        d3 = F.relu(self.bn_dec3(self.dec3(d4)))\n",
        "        d3 = torch.cat([d3, e2], dim=1)  # Skip connection\n",
        "\n",
        "        d2 = F.relu(self.bn_dec2(self.dec2(d3)))\n",
        "        d2 = torch.cat([d2, e1], dim=1)  # Skip connection\n",
        "\n",
        "        d1 = torch.tanh(self.dec1(d2))\n",
        "\n",
        "        return d1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MK9eEkR6yN_P"
      },
      "source": [
        "## 6. Define STFT loss function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "5mItfHi-yORn"
      },
      "outputs": [],
      "source": [
        "class STFTLoss(nn.Module):\n",
        "    def __init__(self, fft_sizes = [512, 1024, 2048, 4096], hop_sizes = [128, 256, 512, 1024], win_lengths = [512, 1024, 2048, 4096], window='hann'):\n",
        "        super(STFTLoss, self).__init__()\n",
        "\n",
        "        # Use multiple FFT sizes for multi-resolution analysis\n",
        "        self.fft_sizes = fft_sizes\n",
        "        self.hop_sizes = hop_sizes\n",
        "        self.win_lengths = win_lengths if win_lengths else fft_sizes\n",
        "\n",
        "        # Register windows for each FFT size\n",
        "        for i in range(len(self.fft_sizes)):\n",
        "            self.register_buffer(f'window_{i}', torch.hann_window(self.win_lengths[i]))\n",
        "\n",
        "    def stft(self, x, fft_size, hop_size, win_length, window):\n",
        "        # Handle different PyTorch versions\n",
        "        if hasattr(torch, 'stft'):\n",
        "            try:\n",
        "                # For PyTorch 1.7+\n",
        "                return torch.stft(\n",
        "                    x, fft_size, hop_size, win_length, window,\n",
        "                    return_complex=True)\n",
        "            except TypeError:\n",
        "                # For older PyTorch versions\n",
        "                stft_result = torch.stft(\n",
        "                    x, fft_size, hop_size, win_length, window)\n",
        "                real, imag = stft_result.unbind(-1)\n",
        "                return torch.complex(real, imag)\n",
        "        else:\n",
        "            raise RuntimeError(\"Current PyTorch version doesn't support torch.stft\")\n",
        "\n",
        "    def compute_spectral_convergence(self, x_mag, y_mag):\n",
        "        return torch.norm(x_mag - y_mag, p='fro') / torch.norm(y_mag, p='fro')\n",
        "\n",
        "    def compute_magnitude_loss(self, x_mag, y_mag):\n",
        "        return torch.mean(torch.abs(x_mag - y_mag))\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # Ensure inputs are the same shape\n",
        "        if x.size() != y.size():\n",
        "            raise ValueError(f\"Inputs must have same size, got {x.size()} and {y.size()}\")\n",
        "\n",
        "        # Ensure inputs are 2D (batch, samples)\n",
        "        if len(x.shape) == 1:\n",
        "            x = x.unsqueeze(0)\n",
        "            y = y.unsqueeze(0)\n",
        "\n",
        "        # Initialize losses\n",
        "        sc_loss = 0.0\n",
        "        mag_loss = 0.0\n",
        "\n",
        "        # Compute loss for each FFT size\n",
        "        for i in range(len(self.fft_sizes)):\n",
        "            x_stft = self.stft(x, self.fft_sizes[i], self.hop_sizes[i],\n",
        "                               self.win_lengths[i], getattr(self, f'window_{i}'))\n",
        "            y_stft = self.stft(y, self.fft_sizes[i], self.hop_sizes[i],\n",
        "                               self.win_lengths[i], getattr(self, f'window_{i}'))\n",
        "\n",
        "            # Compute magnitudes\n",
        "            x_mag = torch.abs(x_stft)\n",
        "            y_mag = torch.abs(y_stft)\n",
        "\n",
        "            # Accumulate losses\n",
        "            sc_loss += self.compute_spectral_convergence(x_mag, y_mag)\n",
        "            mag_loss += self.compute_magnitude_loss(x_mag, y_mag)\n",
        "\n",
        "        # Average over number of FFT sizes\n",
        "        sc_loss = sc_loss / len(self.fft_sizes)\n",
        "        mag_loss = mag_loss / len(self.fft_sizes)\n",
        "\n",
        "        # Total loss (weighted sum)\n",
        "        loss = sc_loss + mag_loss\n",
        "\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv5Ph9BLOfA-"
      },
      "source": [
        "## 7. Training Functions\n",
        "\n",
        "Now let's define functions for training and monitoring our model's progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 578
        },
        "id": "uGIt98SVdoJa",
        "outputId": "c793d783-9c88-471f-df1c-babb9d7717c1"
      },
      "outputs": [],
      "source": [
        "def save_to_file(file_path, train_losses, val_losses, learning_rates):\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "    # Prepare data structure\n",
        "    metrics_data = {\n",
        "        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        \"epochs_completed\": len(train_losses),\n",
        "        \"best_val_loss\": min(val_losses) if val_losses else None,\n",
        "        \"best_epoch\": val_losses.index(min(val_losses)) + 1 if val_losses else None,\n",
        "        \"final_learning_rate\": learning_rates[-1] if learning_rates else None,\n",
        "        \"train_losses\": train_losses,\n",
        "        \"val_losses\": val_losses,\n",
        "        \"learning_rates\": learning_rates\n",
        "    }\n",
        "\n",
        "    # Save to file (with error handling)\n",
        "    try:\n",
        "        with open(file_path, 'w') as f:\n",
        "            json.dump(metrics_data, f, indent=4)\n",
        "        print(f\"Training metrics saved to {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving metrics to {file_path}: {e}\")\n",
        "\n",
        "\n",
        "def plot_training_metrics(metrics_file=None, train_losses=None, val_losses=None, learning_rates=None, output_path=None):\n",
        "    # Load metrics from file if provided\n",
        "    if metrics_file and (train_losses is None or val_losses is None or learning_rates is None):\n",
        "        try:\n",
        "            with open(metrics_file, 'r') as f:\n",
        "                metrics_data = json.load(f)\n",
        "\n",
        "            train_losses = metrics_data.get('train_losses', [])\n",
        "            val_losses = metrics_data.get('val_losses', [])\n",
        "            learning_rates = metrics_data.get('learning_rates', [])\n",
        "\n",
        "            print(f\"Loaded metrics from {metrics_file}\")\n",
        "            print(f\"Epochs: {len(train_losses)}\")\n",
        "            print(f\"Best validation loss: {min(val_losses):.4f} at epoch {val_losses.index(min(val_losses)) + 1}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading metrics from {metrics_file}: {e}\")\n",
        "            return None\n",
        "\n",
        "    # Create figure and subplots\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Generate epochs range\n",
        "    epochs_range = list(range(1, len(train_losses) + 1))\n",
        "\n",
        "    # Plot training and validation loss\n",
        "    ax1.plot(epochs_range, train_losses, 'b-', label='Training Loss')\n",
        "    ax1.plot(epochs_range, val_losses, 'r-', label='Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('STFT Loss')\n",
        "    ax1.set_title('Training and Validation Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # Add annotation for best validation loss\n",
        "    if val_losses:\n",
        "        min_val_epoch = val_losses.index(min(val_losses)) + 1\n",
        "        min_val_loss = min(val_losses)\n",
        "        ax1.annotate(f'Best: {min_val_loss:.4f}',\n",
        "                    xy=(min_val_epoch, min_val_loss),\n",
        "                    xytext=(min_val_epoch, min_val_loss * 1.1),\n",
        "                    arrowprops=dict(facecolor='black', shrink=0.05, width=1.5),\n",
        "                    fontsize=10)\n",
        "\n",
        "    # Plot learning rate\n",
        "    ax2.plot(epochs_range, learning_rates, 'g-')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Learning Rate')\n",
        "    ax2.set_title('Learning Rate Schedule')\n",
        "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
        "    ax2.set_yscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Save figure if output path is provided\n",
        "    if output_path:\n",
        "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "        plt.savefig(output_path)\n",
        "        print(f\"Plot saved to {output_path}\")\n",
        "\n",
        "# Example usage:\n",
        "# 1. Plot from saved file:\n",
        "metrics_file = os.path.join(RESULTS_DIR, \"metrics.json\")\n",
        "output_path  = os.path.join(RESULTS_DIR, \"reconstructed_plot.png\")\n",
        "plot_training_metrics(metrics_file=metrics_file, output_path=output_path)\n",
        "\n",
        "# 2. Plot from provided lists:\n",
        "# plot_training_metrics(train_losses=train_losses, val_losses=val_losses, learning_rates=learning_rates)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "CCx9DmFvOfA-"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, num_epochs=100, learning_rate=0.001,\n",
        "                gradient_accumulation_steps=4, checkpoint=None):\n",
        "    # Set matplotlib to inline mode for notebook display\n",
        "    %matplotlib inline\n",
        "    import torch.nn as nn\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "    # STFT Loss instead of L1 Loss\n",
        "    criterion = STFTLoss(\n",
        "        fft_sizes   = [512, 1024, 2048, 4096],\n",
        "        hop_sizes   = [128, 256, 512, 1024],\n",
        "        win_lengths = [512, 1024, 2048, 4096],\n",
        "        window='hann'\n",
        "    ).to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Initialize training history and start epoch\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    learning_rates = []\n",
        "    start_epoch = 0\n",
        "    best_val_loss = float('inf')\n",
        "\n",
        "    # Load from checkpoint if provided\n",
        "    if checkpoint is not None:\n",
        "        # Load optimizer state\n",
        "        if 'optimizer_state_dict' in checkpoint:\n",
        "            optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "        # Get the epoch we're resuming from\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "        # Load previous losses if available\n",
        "        if 'train_losses' in checkpoint:\n",
        "            train_losses = checkpoint.get('train_losses', [])\n",
        "        if 'val_losses' in checkpoint:\n",
        "            val_losses = checkpoint.get('val_losses', [])\n",
        "\n",
        "        # Load best validation loss if available\n",
        "        if 'best_val_loss' in checkpoint:\n",
        "            best_val_loss = checkpoint['best_val_loss']\n",
        "        elif 'val_loss' in checkpoint:\n",
        "            best_val_loss = checkpoint['val_loss']\n",
        "\n",
        "        # Load learning rates if available\n",
        "        if 'learning_rates' in checkpoint:\n",
        "            learning_rates = checkpoint['learning_rates']\n",
        "\n",
        "        print(f\"Resumed training from epoch {start_epoch}\")\n",
        "        print(f\"Previous best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    # Initialize scheduler (with proper state if resuming)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.5)\n",
        "\n",
        "    # If resuming, update the scheduler with the last validation loss\n",
        "    if checkpoint is not None and len(val_losses) > 0:\n",
        "        scheduler.step(val_losses[-1])\n",
        "\n",
        "    for epoch in range(start_epoch, num_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        for i, (low_res, high_res) in enumerate(tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")):\n",
        "            low_res, high_res = low_res.to(device), high_res.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(low_res)\n",
        "\n",
        "            # Apply STFT loss - ensure outputs and targets are properly shaped\n",
        "            # For waveform processing, we expect [B, T] shape\n",
        "            outputs = outputs.squeeze(1) if outputs.dim() > 2 else outputs\n",
        "            high_res = high_res.squeeze(1) if high_res.dim() > 2 else high_res\n",
        "\n",
        "            loss = criterion(outputs, high_res) / gradient_accumulation_steps\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Update weights after accumulating gradients\n",
        "            if (i + 1) % gradient_accumulation_steps == 0 or (i + 1) == len(train_loader):\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            train_loss += loss.item() * gradient_accumulation_steps\n",
        "\n",
        "            # Clear memory periodically\n",
        "            if i % 50 == 0 and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for low_res, high_res in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} - Validation\"):\n",
        "                low_res, high_res = low_res.to(device), high_res.to(device)\n",
        "\n",
        "                outputs = model(low_res)\n",
        "\n",
        "                # Apply STFT loss - ensure outputs and targets are properly shaped\n",
        "                outputs = outputs.squeeze(1) if outputs.dim() > 2 else outputs\n",
        "                high_res = high_res.squeeze(1) if high_res.dim() > 2 else high_res\n",
        "\n",
        "                loss = criterion(outputs, high_res)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "\n",
        "        val_loss = val_loss / len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        # Track current learning rate\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        learning_rates.append(current_lr)\n",
        "\n",
        "        # Adjust learning rate\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {current_lr:.6f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'learning_rates': learning_rates,\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "            }, os.path.join(RESULTS_DIR, 'best_model.pt'))\n",
        "            print(f\"Saved new best model with validation loss: {val_loss:.4f}\")\n",
        "\n",
        "        # Plot and save loss curves every 5 epochs\n",
        "        if (epoch + 1) % 3 == 0:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'train_losses': train_losses,\n",
        "                'val_losses': val_losses,\n",
        "                'learning_rates': learning_rates,\n",
        "                'best_val_loss': best_val_loss,\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss': val_loss,\n",
        "            }, os.path.join(RESULTS_DIR, f'model_checkpoint_epoch_{epoch+1}.pt'))\n",
        "\n",
        "            # Create and display loss plots\n",
        "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "            # Loss plot\n",
        "            epochs_range = list(range(1, len(train_losses) + 1))\n",
        "            ax1.plot(epochs_range, train_losses, 'b-', label='Training Loss')\n",
        "            ax1.plot(epochs_range, val_losses, 'r-', label='Validation Loss')\n",
        "            ax1.set_xlabel('Epoch')\n",
        "            ax1.set_ylabel('STFT Loss')\n",
        "            ax1.set_title(f'Training and Validation Loss (Epoch {epoch+1})')\n",
        "            ax1.legend()\n",
        "            ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "            # Learning rate plot\n",
        "            ax2.plot(epochs_range, learning_rates, 'g-')\n",
        "            ax2.set_xlabel('Epoch')\n",
        "            ax2.set_ylabel('Learning Rate')\n",
        "            ax2.set_title('Learning Rate Schedule')\n",
        "            ax2.grid(True, linestyle='--', alpha=0.6)\n",
        "            ax2.set_yscale('log')\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            # Save the figure\n",
        "            fig.savefig(os.path.join(RESULTS_DIR, 'loss_curves.png'))\n",
        "            plt.close()\n",
        "\n",
        "            # Collect garbage to free memory\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        save_to_file(os.path.join(RESULTS_DIR, \"metrics.json\"), train_losses, val_losses, learning_rates)\n",
        "\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), os.path.join(RESULTS_DIR, 'audio_sr_final_model.pt'))\n",
        "\n",
        "    # Final plot with all training history\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Final loss plot\n",
        "    epochs_range = list(range(1, len(train_losses) + 1))\n",
        "    ax1.plot(epochs_range, train_losses, 'b-', label='Training Loss')\n",
        "    ax1.plot(epochs_range, val_losses, 'r-', label='Validation Loss')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('STFT Loss')\n",
        "    ax1.set_title('Training and Validation Loss (Final)')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, linestyle='--', alpha=0.6)\n",
        "\n",
        "    # Add min/max annotations\n",
        "    min_val_epoch = val_losses.index(min(val_losses)) + 1\n",
        "    ax1.annotate(f'Best: {min(val_losses):.4f}',\n",
        "                xy=(min_val_epoch, min(val_losses)),\n",
        "                xytext=(min_val_epoch, min(val_losses) * 1.1),\n",
        "                arrowprops=dict(facecolor='black', shrink=0.05, width=1.5),\n",
        "                fontsize=10)\n",
        "\n",
        "    # Learning rate plot\n",
        "    ax2.plot(epochs_range, learning_rates, 'g-')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Learning Rate')\n",
        "    ax2.set_title('Learning Rate Schedule')\n",
        "    ax2.grid(True, linestyle='--', alpha=0.6)\n",
        "    ax2.set_yscale('log')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Save the final figure\n",
        "    fig.savefig(os.path.join(RESULTS_DIR, 'final_training_plots.png'))\n",
        "\n",
        "    print(f\"Training completed! Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    return train_losses, val_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fZ7H8a-OfA_"
      },
      "source": [
        "## 8. Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U0O08Yd4OfA_"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_loader):\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize STFT loss for evaluation\n",
        "    stft_criterion = STFTLoss(\n",
        "        fft_sizes=[1024, 2048, 512],\n",
        "        hop_sizes=[120, 240, 50],\n",
        "        win_lengths=None,\n",
        "        window='hann'\n",
        "    ).to(device)\n",
        "\n",
        "    # Metrics\n",
        "    total_l1_loss = 0.0\n",
        "    total_mse_loss = 0.0\n",
        "    total_stft_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (low_res, high_res) in enumerate(tqdm(test_loader, desc=\"Evaluating\")):\n",
        "            low_res, high_res = low_res.to(device), high_res.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(low_res)\n",
        "\n",
        "            # Ensure correct dimensions for waveform processing\n",
        "            outputs_waveform = outputs.squeeze(1) if outputs.dim() > 2 else outputs\n",
        "            high_res_waveform = high_res.squeeze(1) if high_res.dim() > 2 else high_res\n",
        "\n",
        "            # Calculate metrics\n",
        "            l1_loss = F.l1_loss(outputs_waveform, high_res_waveform)\n",
        "            mse_loss = F.mse_loss(outputs_waveform, high_res_waveform)\n",
        "            stft_loss = stft_criterion(outputs_waveform, high_res_waveform)\n",
        "\n",
        "            total_l1_loss += l1_loss.item()\n",
        "            total_mse_loss += mse_loss.item()\n",
        "            total_stft_loss += stft_loss.item()\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg_l1_loss = total_l1_loss / len(test_loader)\n",
        "    avg_mse_loss = total_mse_loss / len(test_loader)\n",
        "    avg_stft_loss = total_stft_loss / len(test_loader)\n",
        "\n",
        "    print(f\"Test Results:\")\n",
        "    print(f\"- Average L1 Loss: {avg_l1_loss:.4f}\")\n",
        "    print(f\"- Average MSE Loss: {avg_mse_loss:.4f}\")\n",
        "    print(f\"- Average STFT Loss: {avg_stft_loss:.4f}\")\n",
        "\n",
        "    return avg_l1_loss, avg_mse_loss, avg_stft_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRNikHyAOfA_"
      },
      "source": [
        "## 9. Inference Function\n",
        "\n",
        "Let's create a function to enhance new audio files using our trained model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iU-FQv0XOfA_"
      },
      "outputs": [],
      "source": [
        "def enhance_audio(model_path, input_audio_path, output_audio_path):\n",
        "\n",
        "    # Load the trained model\n",
        "    model = AudioUNet()\n",
        "\n",
        "    # Load the checkpoint and extract just the model state dictionary\n",
        "    checkpoint = torch.load(model_path)\n",
        "\n",
        "    # Use the model_state_dict key instead of the whole checkpoint\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    model.eval()\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Load and preprocess the input audio\n",
        "    waveform, sample_rate = torchaudio.load(input_audio_path)\n",
        "\n",
        "    # Convert to mono if stereo\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "    # Resample to the original high sampling rate if necessary\n",
        "    if sample_rate != 44100:\n",
        "        waveform = torchaudio.functional.resample(waveform, sample_rate, 44100)\n",
        "\n",
        "    # Normalize\n",
        "    waveform = waveform / torch.max(torch.abs(waveform))\n",
        "\n",
        "    # Process in chunks to avoid memory issues\n",
        "    chunk_size = 65536  # Adjust based on available memory\n",
        "    enhanced_chunks = []\n",
        "    total_chunks = (waveform.shape[1] + chunk_size - 1) // chunk_size\n",
        "\n",
        "    for i in range(0, waveform.shape[1], chunk_size):\n",
        "        chunk_num = i // chunk_size + 1\n",
        "        chunk = waveform[:, i:i+chunk_size]\n",
        "\n",
        "        # Pad if necessary\n",
        "        if chunk.shape[1] < chunk_size:\n",
        "            padding = chunk_size - chunk.shape[1]\n",
        "            chunk = torch.nn.functional.pad(chunk, (0, padding))\n",
        "\n",
        "        # Move to device\n",
        "        chunk = chunk.to(device)\n",
        "\n",
        "        # Enhance\n",
        "        with torch.no_grad():\n",
        "            enhanced_chunk = model(chunk)\n",
        "\n",
        "        # Remove padding if added\n",
        "        if i + chunk_size > waveform.shape[1]:\n",
        "            original_length = waveform.shape[1] - i\n",
        "            enhanced_chunk = enhanced_chunk[:, :original_length]\n",
        "\n",
        "        # Add to list\n",
        "        enhanced_chunks.append(enhanced_chunk.cpu())\n",
        "\n",
        "    # Concatenate chunks\n",
        "    enhanced_waveform = torch.cat(enhanced_chunks, dim=1)\n",
        "\n",
        "    # Ensure the tensor is 2D [channels, time] before saving\n",
        "    if enhanced_waveform.dim() != 2:\n",
        "        if enhanced_waveform.dim() > 2:\n",
        "            # If it has more than 2 dimensions, flatten all but the first dimension\n",
        "            enhanced_waveform = enhanced_waveform.reshape(enhanced_waveform.shape[0], -1)\n",
        "        elif enhanced_waveform.dim() == 1:\n",
        "            # If it's 1D, add a channel dimension\n",
        "            enhanced_waveform = enhanced_waveform.unsqueeze(0)\n",
        "\n",
        "    # Save the enhanced audio\n",
        "    torchaudio.save(output_audio_path, enhanced_waveform, 44100)\n",
        "\n",
        "    # Return waveforms for visualization\n",
        "    return waveform.cpu(), enhanced_waveform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HW5mPatuOfA_"
      },
      "source": [
        "## 10. Training Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CyVvpsl9uVsG"
      },
      "source": [
        "### 10.1. Split the dataset into train, val and test sets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTl8oRPjOfA_",
        "outputId": "e8bfc77a-2121-4bdd-89e1-637eb9ced157"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0) / 1e9:.2f} GB\")\n",
        "    print(f\"Memory reserved: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"No GPU available, using CPU\")\n",
        "\n",
        "\n",
        "MAX_FILES      = 2000   # size of (train_set + val_set + test_set)\n",
        "BATCH_SIZE     = 32     # Reduce this if you encounter OOM errors\n",
        "SEGMENT_LENGTH = 65536  # Reduce this if you encounter OOM errors\n",
        "\n",
        "# Create dataset splits\n",
        "train_loader, val_loader, test_loader = create_dataset_splits(\n",
        "    DATASET_PATH,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    max_files=MAX_FILES,\n",
        "    segment_length=SEGMENT_LENGTH\n",
        ")\n",
        "\n",
        "print(f\"Dataset ready: {len(train_loader)} training batches, {len(val_loader)} validation batches\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlrdPTx7uq-P"
      },
      "source": [
        "### 10.2. Function definitions for saving and loading dataset split for future use when resuming training from checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zrS72K4ftLd-"
      },
      "outputs": [],
      "source": [
        "DATASET_SAVE_PATH = os.path.join(RESULTS_DIR, \"dataset_splits.pkl\") # useful when training from chekpoint and using the same splits\n",
        "\n",
        "# Load dataset splits from a file if it exists.\n",
        "def load_dataset_splits(save_path=DATASET_SAVE_PATH):\n",
        "    if os.path.exists(save_path):\n",
        "        print(f\"Loading dataset splits from {save_path}\")\n",
        "        with open(save_path, 'rb') as f:\n",
        "            dataset_info = pickle.load(f)\n",
        "\n",
        "        # Recreate the DataLoaders with the saved datasets\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            dataset_info['train_dataset'],\n",
        "            batch_size=dataset_info['batch_size'],\n",
        "            shuffle=True,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            dataset_info['val_dataset'],\n",
        "            batch_size=dataset_info['batch_size'],\n",
        "            shuffle=False,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            dataset_info['test_dataset'],\n",
        "            batch_size=dataset_info['batch_size'],\n",
        "            shuffle=False,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        return train_loader, val_loader, test_loader, True\n",
        "\n",
        "    return None, None, None, False\n",
        "\n",
        "# Save dataset splits to a file for later reuse.\n",
        "def save_dataset_splits(train_loader, val_loader, test_loader, save_path=DATASET_SAVE_PATH):\n",
        "    # Ensure the directory exists\n",
        "    os.makedirs(os.path.dirname(save_path) if os.path.dirname(save_path) else '.', exist_ok=True)\n",
        "\n",
        "    # Save the dataset parameters\n",
        "    dataset_info = {\n",
        "        'train_dataset': train_loader.dataset,\n",
        "        'val_dataset': val_loader.dataset,\n",
        "        'test_dataset': test_loader.dataset,\n",
        "        'batch_size': train_loader.batch_size,\n",
        "        'segment_length': getattr(train_loader.dataset, 'segment_length', None)\n",
        "    }\n",
        "\n",
        "    with open(save_path, 'wb') as f:\n",
        "        pickle.dump(dataset_info, f)\n",
        "\n",
        "    print(f\"Dataset splits saved to {save_path}\")\n",
        "    return save_path\n",
        "\n",
        "\n",
        "# Save the dataset splits for future use\n",
        "# save_dataset_splits(train_loader, val_loader, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmWrA4CovCCy"
      },
      "source": [
        "### 10.3. Start training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "P7oZnYKHOfA_",
        "outputId": "a753c357-fc37-4830-94b1-26ef77ac0bef"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = AudioUNet()\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Train model\n",
        "train_losses, val_losses = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    num_epochs=100,  # You can adjust this\n",
        "    learning_rate=0.0010,\n",
        "    gradient_accumulation_steps=4  # Helps with memory issues\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nzpo9vjf2aw_"
      },
      "source": [
        "### 10.4. (optional) Resume training from a specific checkpoint.\n",
        "Files required:\n",
        " - `model_checkpoint.pt`\n",
        " - `previous_dataset_split.pkl`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKcf_bLe8rfX",
        "outputId": "b8099927-2f07-4d68-f4bd-2e3179a5d009"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = AudioUNet()\n",
        "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "# Checkpoint path - specify the checkpoint to resume from\n",
        "checkpoint_path = os.path.join(RESULTS_DIR, \"model_checkpoint_epoch_39.pt\")\n",
        "\n",
        "# Check if checkpoint exists\n",
        "if os.path.exists(checkpoint_path):\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "    checkpoint = torch.load(checkpoint_path, map_location='cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    print(f\"Resuming from epoch {checkpoint['epoch'] + 1}\")\n",
        "else:\n",
        "    print(\"No checkpoint found. Starting training from scratch.\")\n",
        "    checkpoint = None\n",
        "\n",
        "\n",
        "train_loader, val_loader, test_loader, loaded = load_dataset_splits(DATASET_SAVE_PATH)\n",
        "if not loaded:\n",
        "    print(\"Failed to load dataset, please check the file path\")\n",
        "\n",
        "# Train model\n",
        "train_losses, val_losses = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    num_epochs=110, # Remember to pass a higher epoch number\n",
        "    gradient_accumulation_steps=4,\n",
        "    checkpoint=checkpoint  # Pass the loaded checkpoint\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJJng1e6OfA_"
      },
      "source": [
        "## 11. Evaluate model on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rEapmzk-OfA_",
        "outputId": "dd2a1b9c-1e30-4324-fcda-8a8adb314f6c"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "best_model = AudioUNet()\n",
        "checkpoint = torch.load(os.path.join(RESULTS_DIR, 'best_model.pt'))#, map_location='cpu')\n",
        "best_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Evaluate model\n",
        "l1_loss, mse_loss, stft_loss = evaluate_model(best_model, test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFk5RW29OfA_"
      },
      "source": [
        "## 12. Inference on new audio files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "IsF3OOLDOfBA",
        "outputId": "5f1aac3f-ee8e-4bcd-c6b8-06a128602c4f"
      },
      "outputs": [],
      "source": [
        "# Example of using the enhance_audio function\n",
        "input_path_hq = '/content/hq-track.mp3'  # Change this to your input file\n",
        "input_path_lq = \"/content/lq-track.mp3\"\n",
        "output_path = '/content/enhanced.wav'\n",
        "\n",
        "# Downsample the hq track\n",
        "\n",
        "# Load the audio\n",
        "waveform, sample_rate = torchaudio.load(input_path_hq)\n",
        "# Convert to mono if stereo\n",
        "if waveform.shape[0] > 1:\n",
        "    waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "# Create low resolution version with proper low-pass filtering\n",
        "\n",
        "# 1. Downsample to low SR\n",
        "waveform_low = torchaudio.functional.resample(waveform, sample_rate, 16000)\n",
        "\n",
        "# 2. Upsample back to original SR\n",
        "waveform_low = torchaudio.functional.resample(waveform_low, 16000, 44100)\n",
        "\n",
        "# 3. Apply low-pass filter to simulate bandwidth limitation\n",
        "cutoff_freq = 16000 / 2 * 0.6  # 60% of Nyquist frequency for the low sample rate\n",
        "waveform_low = torchaudio.functional.lowpass_biquad(\n",
        "    waveform_low,\n",
        "    44100,\n",
        "    cutoff_freq\n",
        ")\n",
        "\n",
        "torchaudio.save(input_path_lq, waveform_low, 44100)\n",
        "\n",
        "# Uncomment to enhance an audio file\n",
        "original, enhanced = enhance_audio(\n",
        "    model_path=os.path.join(RESULTS_DIR, 'best_model.pt'),\n",
        "    input_audio_path=input_path_lq,\n",
        "    output_audio_path=output_path\n",
        ")\n",
        "\n",
        "# Visualize original and enhanced waveforms\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.plot(original.numpy()[0])\n",
        "plt.title('Original Audio')\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.plot(enhanced.numpy()[0])\n",
        "plt.title('Enhanced Audio')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
